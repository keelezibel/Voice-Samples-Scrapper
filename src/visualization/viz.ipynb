{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tensorboard\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "\n",
    "from glob import glob\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME=\"runs/spkr_ident_ref\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(RUN_NAME)\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(filepath):\n",
    "    classifier = EncoderClassifier.from_hparams(\n",
    "            source=r\"../../../models/sb_ecapa_pretrained\",\n",
    "            savedir=r\"../../../models/sb_ecapa_pretrained\",\n",
    "        )\n",
    "    signal, _ = torchaudio.load(filepath)\n",
    "    embeddings = classifier.encode_batch(signal)\n",
    "    embeddings = torch.flatten(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_all_files(dir):\n",
    "    embeddings = []\n",
    "    spkrs = []\n",
    "    audio_files = glob(dir)\n",
    "    for file in tqdm(audio_files):\n",
    "        file_dir = os.path.dirname(file)\n",
    "        spkr_name = file_dir.split(os.path.sep)[-1]  # Spkr Name (str)\n",
    "        embedding = get_embeddings(file) # Tensors \n",
    "        \n",
    "        embeddings.append(embedding)\n",
    "        spkrs.append(spkr_name)\n",
    "    return spkrs, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files_dir = r\"../../data/ref_audio/**/*.wav\"\n",
    "spkrs, embeddings = iterate_all_files(audio_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_reshape = torch.stack(embeddings, 0)\n",
    "embeddings_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spkrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_embedding(embeddings_reshape, spkrs)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-de0aa095c8fc27e4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-de0aa095c8fc27e4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir $RUN_NAME --port=6008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_pil = []\n",
    "audio_embeddings = []\n",
    "labels = []\n",
    "\n",
    "img_path = \"../../data/ref_images/anwar.jpg\"\n",
    "img_tf = get_img(img_path)\n",
    "# Save both tf image for prediction and PIL image for sprite\n",
    "# img_pil = Image.fromarray(np.uint8(img_tf.numpy())*255).resize((100, 100))\n",
    "img_pil = img_tf.numpy()\n",
    "# write to tensorboard\n",
    "writer.add_image('test', img_pil)\n",
    "\n",
    "# audio_embedding = get_embeddings(\"../../data/ref_audio/YB DATO SERI ANWAR BIN IBRAHIM/NQZap4wJ3rU.wav\")\n",
    "# audio_embeddings.append(audio_embedding.numpy()[0])\n",
    "# images_pil.append(img_pil)\n",
    "# # Assuming your output data is directly the label\n",
    "# label = \"Anwar\" \n",
    "# labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize_with_pad(img, 100, 100)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "tensorboard",
   "language": "python",
   "name": "tensorboard"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd43712a03b5d2d18ef0e4f5f9be720625180271fa0382392d3a5025a45202f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
